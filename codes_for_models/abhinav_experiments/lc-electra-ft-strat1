Sender: LSF System <lsfadmin@eu-g3-054>
Subject: Job 203307726: <python logicclimate.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -mp masked-logical-form -ts 1 -ds 1 -f T -s /cluster/project/sachan/abhinav/saved_models/electra-lc-ft-strat1> in cluster <euler> Done

Job <python logicclimate.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -mp masked-logical-form -ts 1 -ds 1 -f T -s /cluster/project/sachan/abhinav/saved_models/electra-lc-ft-strat1> was submitted from host <eu-login-01> by user <alalwani> in cluster <euler> at Fri Feb  4 12:02:19 2022
Job was executed on host(s) <eu-g3-054>, in queue <gpuhe.4h>, as user <alalwani> in cluster <euler> at Fri Feb  4 12:02:36 2022
</cluster/home/alalwani> was used as the home directory.
</cluster/home/alalwani/logical-fallacy/codes_for_models/abhinav_experiments> was used as the working directory.
Started at Fri Feb  4 12:02:36 2022
Terminated at Fri Feb  4 12:35:31 2022
Results reported at Fri Feb  4 12:35:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python logicclimate.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -mp masked-logical-form -ts 1 -ds 1 -f T -s /cluster/project/sachan/abhinav/saved_models/electra-lc-ft-strat1
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1928.40 sec.
    Max Memory :                                 7087 MB
    Average Memory :                             6129.98 MB
    Total Requested Memory :                     30000.00 MB
    Delta Memory :                               22913.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                9
    Run time :                                   1975 sec.
    Turnaround time :                            1992 sec.

The output (if any) follows:

device = cuda
Namespace(tokenizer='/cluster/project/sachan/abhinav/saved_models/electra-mnli', model='/cluster/project/sachan/abhinav/saved_models/electra-mnli', weight='12', savepath='/cluster/project/sachan/abhinav/saved_models/electra-lc-ft-strat1', map='masked-logical-form', train_strat='1', dev_strat='1', finetune='T')
initializing model
creating dataset
starting training
0 0
0 10
0 20
0 30
0 40
0 50
0 60
0 70
0 80
0 90
0 100
0 110
0 120
0 130
0 140
0 150
0 160
0 170
0 180
0 190
0 200
0 210
0 220
0 230
0 240
0 250
0 260
0 270
0 280
0 290
0 300
0 310
0 320
0 330
0 340
0 350
0 360
0 370
saving model
Epoch 1: train_loss: 0.6690 train_acc: 0.7474 train_prec: 0.1272 train_rec: 0.3578| val_loss: 0.6457 val_acc: 0.5685 val_prec: 0.1175 val_rec: 0.6754
00:05:18.69
1 0
1 10
1 20
1 30
1 40
1 50
1 60
1 70
1 80
1 90
1 100
1 110
1 120
1 130
1 140
1 150
1 160
1 170
1 180
1 190
1 200
1 210
1 220
1 230
1 240
1 250
1 260
1 270
1 280
1 290
1 300
1 310
1 320
1 330
1 340
1 350
1 360
1 370
saving model
Epoch 2: train_loss: 0.6393 train_acc: 0.7215 train_prec: 0.1570 train_rec: 0.4386| val_loss: 0.6332 val_acc: 0.6310 val_prec: 0.1299 val_rec: 0.6183
00:05:20.01
2 0
2 10
2 20
2 30
2 40
2 50
2 60
2 70
2 80
2 90
2 100
2 110
2 120
2 130
2 140
2 150
2 160
2 170
2 180
2 190
2 200
2 210
2 220
2 230
2 240
2 250
2 260
2 270
2 280
2 290
2 300
2 310
2 320
2 330
2 340
2 350
2 360
2 370
saving model
Epoch 3: train_loss: 0.6271 train_acc: 0.7338 train_prec: 0.1557 train_rec: 0.4487| val_loss: 0.6313 val_acc: 0.8835 val_prec: 0.2455 val_rec: 0.2647
00:05:19.64
3 0
3 10
3 20
3 30
3 40
3 50
3 60
3 70
3 80
3 90
3 100
3 110
3 120
3 130
3 140
3 150
3 160
3 170
3 180
3 190
3 200
3 210
3 220
3 230
3 240
3 250
3 260
3 270
3 280
3 290
3 300
3 310
3 320
3 330
3 340
3 350
3 360
3 370
saving model
Epoch 4: train_loss: 0.6190 train_acc: 0.7432 train_prec: 0.1671 train_rec: 0.4515| val_loss: 0.6267 val_acc: 0.5765 val_prec: 0.1205 val_rec: 0.6323
00:05:18.41
4 0
4 10
4 20
4 30
4 40
4 50
4 60
4 70
4 80
4 90
4 100
4 110
4 120
4 130
4 140
4 150
4 160
4 170
4 180
4 190
4 200
4 210
4 220
4 230
4 240
4 250
4 260
4 270
4 280
4 290
4 300
4 310
4 320
4 330
4 340
4 350
4 360
4 370
saving model
Epoch 5: train_loss: 0.5928 train_acc: 0.7263 train_prec: 0.1649 train_rec: 0.5425| val_loss: 0.5980 val_acc: 0.7785 val_prec: 0.1826 val_rec: 0.4928
00:05:18.49
5 0
5 10
5 20
5 30
5 40
5 50
5 60
5 70
5 80
5 90
5 100
5 110
5 120
5 130
5 140
5 150
5 160
5 170
5 180
5 190
5 200
5 210
5 220
5 230
5 240
5 250
5 260
5 270
5 280
5 290
5 300
5 310
5 320
5 330
5 340
5 350
5 360
5 370
Epoch 6: train_loss: 0.5750 train_acc: 0.7207 train_prec: 0.1761 train_rec: 0.5735| val_loss: 0.6117 val_acc: 0.7652 val_prec: 0.1800 val_rec: 0.5843
00:05:14.53
starting testing
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
micro f1: 0.276786 macro f1:0.119286 precision: 0.204971 recall: 0.543860 exact match 0.035088
['intentional', 'fallacy of logic', 'faulty generalization', 'equivocation', 'appeal to emotion', 'false dilemma', 'fallacy of extension', 'circular reasoning', 'fallacy of relevance', 'false causality', 'ad hominem', 'ad populum', 'fallacy of credibility']
