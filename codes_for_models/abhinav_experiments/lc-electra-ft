Sender: LSF System <lsfadmin@eu-g3-056>
Subject: Job 203265992: <python logicclimate.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -mp base -ts 1 -ds 1 -f T -s /cluster/project/sachan/abhinav/saved_models/electra-lc-ft> in cluster <euler> Done

Job <python logicclimate.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -mp base -ts 1 -ds 1 -f T -s /cluster/project/sachan/abhinav/saved_models/electra-lc-ft> was submitted from host <eu-login-22> by user <alalwani> in cluster <euler> at Thu Feb  3 23:37:13 2022
Job was executed on host(s) <eu-g3-056>, in queue <gpuhe.4h>, as user <alalwani> in cluster <euler> at Thu Feb  3 23:37:31 2022
</cluster/home/alalwani> was used as the home directory.
</cluster/home/alalwani/logical-fallacy/codes_for_models/abhinav_experiments> was used as the working directory.
Started at Thu Feb  3 23:37:31 2022
Terminated at Fri Feb  4 00:03:02 2022
Results reported at Fri Feb  4 00:03:02 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python logicclimate.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -mp base -ts 1 -ds 1 -f T -s /cluster/project/sachan/abhinav/saved_models/electra-lc-ft
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1488.65 sec.
    Max Memory :                                 7071 MB
    Average Memory :                             6055.21 MB
    Total Requested Memory :                     30000.00 MB
    Delta Memory :                               22929.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                9
    Run time :                                   1530 sec.
    Turnaround time :                            1549 sec.

The output (if any) follows:

device = cuda
Namespace(tokenizer='/cluster/project/sachan/abhinav/saved_models/electra-mnli', model='/cluster/project/sachan/abhinav/saved_models/electra-mnli', weight='12', savepath='/cluster/project/sachan/abhinav/saved_models/electra-lc-ft', map='base', train_strat='1', dev_strat='1', finetune='T')
initializing model
creating dataset
starting training
0 0
0 10
0 20
0 30
0 40
0 50
0 60
0 70
0 80
0 90
0 100
0 110
0 120
0 130
0 140
0 150
0 160
0 170
0 180
0 190
0 200
0 210
0 220
0 230
0 240
0 250
0 260
0 270
0 280
0 290
0 300
0 310
0 320
0 330
0 340
0 350
0 360
0 370
saving model
Epoch 1: train_loss: 0.6976 train_acc: 0.7780 train_prec: 0.0646 train_rec: 0.2026| val_loss: 0.6509 val_acc: 0.7715 val_prec: 0.1513 val_rec: 0.3556
00:04:55.00
1 0
1 10
1 20
1 30
1 40
1 50
1 60
1 70
1 80
1 90
1 100
1 110
1 120
1 130
1 140
1 150
1 160
1 170
1 180
1 190
1 200
1 210
1 220
1 230
1 240
1 250
1 260
1 270
1 280
1 290
1 300
1 310
1 320
1 330
1 340
1 350
1 360
1 370
saving model
Epoch 2: train_loss: 0.6523 train_acc: 0.7716 train_prec: 0.1693 train_rec: 0.3699| val_loss: 0.6341 val_acc: 0.8114 val_prec: 0.1841 val_rec: 0.3406
00:04:55.66
2 0
2 10
2 20
2 30
2 40
2 50
2 60
2 70
2 80
2 90
2 100
2 110
2 120
2 130
2 140
2 150
2 160
2 170
2 180
2 190
2 200
2 210
2 220
2 230
2 240
2 250
2 260
2 270
2 280
2 290
2 300
2 310
2 320
2 330
2 340
2 350
2 360
2 370
saving model
Epoch 3: train_loss: 0.6310 train_acc: 0.7883 train_prec: 0.1657 train_rec: 0.3865| val_loss: 0.6262 val_acc: 0.5066 val_prec: 0.1158 val_rec: 0.7587
00:04:55.71
3 0
3 10
3 20
3 30
3 40
3 50
3 60
3 70
3 80
3 90
3 100
3 110
3 120
3 130
3 140
3 150
3 160
3 170
3 180
3 190
3 200
3 210
3 220
3 230
3 240
3 250
3 260
3 270
3 280
3 290
3 300
3 310
3 320
3 330
3 340
3 350
3 360
3 370
saving model
Epoch 4: train_loss: 0.6233 train_acc: 0.7629 train_prec: 0.1782 train_rec: 0.4055| val_loss: 0.6128 val_acc: 0.8318 val_prec: 0.1927 val_rec: 0.2968
00:04:55.64
4 0
4 10
4 20
4 30
4 40
4 50
4 60
4 70
4 80
4 90
4 100
4 110
4 120
4 130
4 140
4 150
4 160
4 170
4 180
4 190
4 200
4 210
4 220
4 230
4 240
4 250
4 260
4 270
4 280
4 290
4 300
4 310
4 320
4 330
4 340
4 350
4 360
4 370
Epoch 5: train_loss: 0.6309 train_acc: 0.7407 train_prec: 0.1481 train_rec: 0.4070| val_loss: 0.6498 val_acc: 0.4136 val_prec: 0.0995 val_rec: 0.7568
00:04:51.61
starting testing
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
micro f1: 0.276692 macro f1:0.055596 precision: 0.216192 recall: 0.403509 exact match 0.043860
['appeal to emotion', 'fallacy of relevance', 'ad hominem', 'circular reasoning', 'equivocation', 'false causality', 'fallacy of credibility', 'faulty generalization', 'fallacy of logic', 'intentional', 'fallacy of extension', 'ad populum', 'false dilemma']
