Sender: LSF System <lsfadmin@eu-g3-061>
Subject: Job 203249150: <python logicedu.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -s /cluster/project/sachan/abhinav/saved_models/electra-ft -mp base -ts 1 -ds 1> in cluster <euler> Exited

Job <python logicedu.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -s /cluster/project/sachan/abhinav/saved_models/electra-ft -mp base -ts 1 -ds 1> was submitted from host <eu-login-32> by user <alalwani> in cluster <euler> at Thu Feb  3 17:53:33 2022
Job was executed on host(s) <eu-g3-061>, in queue <gpuhe.4h>, as user <alalwani> in cluster <euler> at Thu Feb  3 17:53:48 2022
</cluster/home/alalwani> was used as the home directory.
</cluster/home/alalwani/logical-fallacy/codes_for_models/abhinav_experiments> was used as the working directory.
Started at Thu Feb  3 17:53:48 2022
Terminated at Thu Feb  3 19:24:56 2022
Results reported at Thu Feb  3 19:24:56 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python logicedu.py -t /cluster/project/sachan/abhinav/saved_models/electra-mnli -m /cluster/project/sachan/abhinav/saved_models/electra-mnli -w 12 -s /cluster/project/sachan/abhinav/saved_models/electra-ft -mp base -ts 1 -ds 1
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   5434.80 sec.
    Max Memory :                                 6410 MB
    Average Memory :                             6219.67 MB
    Total Requested Memory :                     30000.00 MB
    Delta Memory :                               23590.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                9
    Run time :                                   5468 sec.
    Turnaround time :                            5483 sec.

The output (if any) follows:

device = cuda
initializing model
creating dataset
starting training
0 0
0 10
0 20
0 30
0 40
0 50
0 60
0 70
0 80
0 90
0 100
0 110
0 120
0 130
0 140
0 150
0 160
0 170
0 180
0 190
0 200
0 210
0 220
0 230
0 240
0 250
0 260
0 270
0 280
0 290
0 300
0 310
0 320
0 330
0 340
0 350
0 360
0 370
0 380
0 390
0 400
0 410
0 420
0 430
0 440
0 450
0 460
0 470
0 480
0 490
0 500
0 510
0 520
0 530
0 540
0 550
0 560
0 570
0 580
0 590
0 600
0 610
0 620
0 630
0 640
0 650
0 660
0 670
0 680
0 690
0 700
0 710
0 720
0 730
0 740
0 750
saving model
Epoch 1: train_loss: 0.6888 train_acc: 0.7693 train_prec: 0.0807 train_rec: 0.2334| val_loss: 0.6393 val_acc: 0.8600 val_prec: 0.2154 val_rec: 0.3040
00:05:47.65
1 0
1 10
1 20
1 30
1 40
1 50
1 60
1 70
1 80
1 90
1 100
1 110
1 120
1 130
1 140
1 150
1 160
1 170
1 180
1 190
1 200
1 210
1 220
1 230
1 240
1 250
1 260
1 270
1 280
1 290
1 300
1 310
1 320
1 330
1 340
1 350
1 360
1 370
1 380
1 390
1 400
1 410
1 420
1 430
1 440
1 450
1 460
1 470
1 480
1 490
1 500
1 510
1 520
1 530
1 540
1 550
1 560
1 570
1 580
1 590
1 600
1 610
1 620
1 630
1 640
1 650
1 660
1 670
1 680
1 690
1 700
1 710
1 720
1 730
1 740
1 750
saving model
Epoch 2: train_loss: 0.5551 train_acc: 0.7933 train_prec: 0.2214 train_rec: 0.5484| val_loss: 0.4203 val_acc: 0.8341 val_prec: 0.2850 val_rec: 0.7522
00:05:48.10
2 0
2 10
2 20
2 30
2 40
2 50
2 60
2 70
2 80
2 90
2 100
2 110
2 120
2 130
2 140
2 150
2 160
2 170
2 180
2 190
2 200
2 210
2 220
2 230
2 240
2 250
2 260
2 270
2 280
2 290
2 300
2 310
2 320
2 330
2 340
2 350
2 360
2 370
2 380
2 390
2 400
2 410
2 420
2 430
2 440
2 450
2 460
2 470
2 480
2 490
2 500
2 510
2 520
2 530
2 540
2 550
2 560
2 570
2 580
2 590
2 600
2 610
2 620
2 630
2 640
2 650
2 660
2 670
2 680
2 690
2 700
2 710
2 720
2 730
2 740
2 750
saving model
Epoch 3: train_loss: 0.3639 train_acc: 0.8754 train_prec: 0.3672 train_rec: 0.7198| val_loss: 0.3412 val_acc: 0.9052 val_prec: 0.4127 val_rec: 0.6911
00:05:48.14
3 0
3 10
3 20
3 30
3 40
3 50
3 60
3 70
3 80
3 90
3 100
3 110
3 120
3 130
3 140
3 150
3 160
3 170
3 180
3 190
3 200
3 210
3 220
3 230
3 240
3 250
3 260
3 270
3 280
3 290
3 300
3 310
3 320
3 330
3 340
3 350
3 360
3 370
3 380
3 390
3 400
3 410
3 420
3 430
3 440
3 450
3 460
3 470
3 480
3 490
3 500
3 510
3 520
3 530
3 540
3 550
3 560
3 570
3 580
3 590
3 600
3 610
3 620
3 630
3 640
3 650
3 660
3 670
3 680
3 690
3 700
3 710
3 720
3 730
3 740
3 750
Epoch 4: train_loss: 0.2280 train_acc: 0.9257 train_prec: 0.5127 train_rec: 0.8168| val_loss: 0.3865 val_acc: 0.9077 val_prec: 0.4225 val_rec: 0.7105
00:05:44.21
4 0
4 10
4 20
4 30
4 40
4 50
4 60
4 70
4 80
4 90
4 100
4 110
4 120
4 130
4 140
4 150
4 160
4 170
4 180
4 190
4 200
4 210
4 220
4 230
4 240
4 250
4 260
4 270
4 280
4 290
4 300
4 310
4 320
4 330
4 340
4 350
4 360
4 370
4 380
4 390
4 400
4 410
4 420
4 430
4 440
4 450
4 460
4 470
4 480
4 490
4 500
4 510
4 520
4 530
4 540
4 550
4 560
4 570
4 580
4 590
4 600
4 610
4 620
4 630
4 640
4 650
4 660
4 670
4 680
4 690
4 700
4 710
4 720
4 730
4 740
4 750
Epoch 5: train_loss: 0.1418 train_acc: 0.9521 train_prec: 0.6198 train_rec: 0.8716| val_loss: 0.3480 val_acc: 0.9159 val_prec: 0.4810 val_rec: 0.7478
00:05:44.34
5 0
5 10
5 20
5 30
5 40
5 50
5 60
5 70
5 80
5 90
5 100
5 110
5 120
5 130
5 140
5 150
5 160
5 170
5 180
5 190
5 200
5 210
5 220
5 230
5 240
5 250
5 260
5 270
5 280
5 290
5 300
5 310
5 320
5 330
5 340
5 350
5 360
5 370
5 380
5 390
5 400
5 410
5 420
5 430
5 440
5 450
5 460
5 470
5 480
5 490
5 500
5 510
5 520
5 530
5 540
5 550
5 560
5 570
5 580
5 590
5 600
5 610
5 620
5 630
5 640
5 650
5 660
5 670
5 680
5 690
5 700
5 710
5 720
5 730
5 740
5 750
Epoch 6: train_loss: 0.0892 train_acc: 0.9695 train_prec: 0.7025 train_rec: 0.8797| val_loss: 0.6242 val_acc: 0.9492 val_prec: 0.5959 val_rec: 0.6026
00:05:44.36
6 0
6 10
6 20
6 30
6 40
6 50
6 60
6 70
6 80
6 90
6 100
6 110
6 120
6 130
6 140
6 150
6 160
6 170
6 180
6 190
6 200
6 210
6 220
6 230
6 240
6 250
6 260
6 270
6 280
6 290
6 300
6 310
6 320
6 330
6 340
6 350
6 360
6 370
6 380
6 390
6 400
6 410
6 420
6 430
6 440
6 450
6 460
6 470
6 480
6 490
6 500
6 510
6 520
6 530
6 540
6 550
6 560
6 570
6 580
6 590
6 600
6 610
6 620
6 630
6 640
6 650
6 660
6 670
6 680
6 690
6 700
6 710
6 720
6 730
6 740
6 750
Epoch 7: train_loss: 0.0739 train_acc: 0.9739 train_prec: 0.7310 train_rec: 0.8962| val_loss: 0.6345 val_acc: 0.9387 val_prec: 0.5458 val_rec: 0.6627
00:05:44.20
7 0
7 10
7 20
7 30
7 40
7 50
7 60
7 70
7 80
7 90
7 100
7 110
7 120
7 130
7 140
7 150
7 160
7 170
7 180
7 190
7 200
7 210
7 220
7 230
7 240
7 250
7 260
7 270
7 280
7 290
7 300
7 310
7 320
7 330
7 340
7 350
7 360
7 370
7 380
7 390
7 400
7 410
7 420
7 430
7 440
7 450
7 460
7 470
7 480
7 490
7 500
7 510
7 520
7 530
7 540
7 550
7 560
7 570
7 580
7 590
7 600
7 610
7 620
7 630
7 640
7 650
7 660
7 670
7 680
7 690
7 700
7 710
7 720
7 730
7 740
7 750
Epoch 8: train_loss: 0.0590 train_acc: 0.9832 train_prec: 0.7928 train_rec: 0.8980| val_loss: 0.5779 val_acc: 0.9472 val_prec: 0.6227 val_rec: 0.6723
00:05:44.20
8 0
8 10
8 20
8 30
8 40
8 50
8 60
8 70
8 80
8 90
8 100
8 110
8 120
8 130
8 140
8 150
8 160
8 170
8 180
8 190
8 200
8 210
8 220
8 230
8 240
8 250
8 260
8 270
8 280
8 290
8 300
8 310
8 320
8 330
8 340
8 350
8 360
8 370
8 380
8 390
8 400
8 410
8 420
8 430
8 440
8 450
8 460
8 470
8 480
8 490
8 500
8 510
8 520
8 530
8 540
8 550
8 560
8 570
8 580
8 590
8 600
8 610
8 620
8 630
8 640
8 650
8 660
8 670
8 680
8 690
8 700
8 710
8 720
8 730
8 740
8 750
Epoch 9: train_loss: 0.0359 train_acc: 0.9872 train_prec: 0.8153 train_rec: 0.9082| val_loss: 1.0215 val_acc: 0.9410 val_prec: 0.5735 val_rec: 0.6614
00:05:44.30
9 0
9 10
9 20
9 30
9 40
9 50
9 60
9 70
9 80
9 90
9 100
9 110
9 120
9 130
9 140
9 150
9 160
9 170
9 180
9 190
9 200
9 210
9 220
9 230
9 240
9 250
9 260
9 270
9 280
9 290
9 300
9 310
9 320
9 330
9 340
9 350
9 360
9 370
9 380
9 390
9 400
9 410
9 420
9 430
9 440
9 450
9 460
9 470
9 480
9 490
9 500
9 510
9 520
9 530
9 540
9 550
9 560
9 570
9 580
9 590
9 600
9 610
9 620
9 630
9 640
9 650
9 660
9 670
9 680
9 690
9 700
9 710
9 720
9 730
9 740
9 750
Epoch 10: train_loss: 0.0405 train_acc: 0.9876 train_prec: 0.8253 train_rec: 0.9003| val_loss: 0.7134 val_acc: 0.9228 val_prec: 0.4862 val_rec: 0.6840
00:05:44.35
10 0
10 10
10 20
10 30
10 40
10 50
10 60
10 70
10 80
10 90
10 100
10 110
10 120
10 130
10 140
10 150
10 160
10 170
10 180
10 190
10 200
10 210
10 220
10 230
10 240
10 250
10 260
10 270
10 280
10 290
10 300
10 310
10 320
10 330
10 340
10 350
10 360
10 370
10 380
10 390
10 400
10 410
10 420
10 430
10 440
10 450
10 460
10 470
10 480
10 490
10 500
10 510
10 520
10 530
10 540
10 550
10 560
10 570
10 580
10 590
10 600
10 610
10 620
10 630
10 640
10 650
10 660
10 670
10 680
10 690
10 700
10 710
10 720
10 730
10 740
10 750
Epoch 11: train_loss: 0.0323 train_acc: 0.9882 train_prec: 0.8327 train_rec: 0.9193| val_loss: 0.7572 val_acc: 0.9441 val_prec: 0.5608 val_rec: 0.6430
00:05:44.26
11 0
11 10
11 20
11 30
11 40
11 50
11 60
11 70
11 80
11 90
11 100
11 110
11 120
11 130
11 140
11 150
11 160
11 170
11 180
11 190
11 200
11 210
11 220
11 230
11 240
11 250
11 260
11 270
11 280
11 290
11 300
11 310
11 320
11 330
11 340
11 350
11 360
11 370
11 380
11 390
11 400
11 410
11 420
11 430
11 440
11 450
11 460
11 470
11 480
11 490
11 500
11 510
11 520
11 530
11 540
11 550
11 560
11 570
11 580
11 590
11 600
11 610
11 620
11 630
11 640
11 650
11 660
11 670
11 680
11 690
11 700
11 710
11 720
11 730
11 740
11 750
Epoch 12: train_loss: 0.0260 train_acc: 0.9922 train_prec: 0.8685 train_rec: 0.9192| val_loss: 1.2049 val_acc: 0.9550 val_prec: 0.6723 val_rec: 0.6335
00:05:44.33
12 0
12 10
12 20
12 30
12 40
12 50
12 60
12 70
12 80
12 90
12 100
12 110
12 120
12 130
12 140
12 150
12 160
12 170
12 180
12 190
12 200
12 210
12 220
12 230
12 240
12 250
12 260
12 270
12 280
12 290
12 300
12 310
12 320
12 330
12 340
12 350
12 360
12 370
12 380
12 390
12 400
12 410
12 420
12 430
12 440
12 450
12 460
12 470
12 480
12 490
12 500
12 510
12 520
12 530
12 540
12 550
12 560
12 570
12 580
12 590
12 600
12 610
12 620
12 630
12 640
12 650
12 660
12 670
12 680
12 690
12 700
12 710
12 720
12 730
12 740
12 750
Epoch 13: train_loss: 0.0391 train_acc: 0.9891 train_prec: 0.8543 train_rec: 0.9252| val_loss: 0.6296 val_acc: 0.9372 val_prec: 0.5508 val_rec: 0.6414
00:05:44.47
13 0
13 10
13 20
13 30
13 40
13 50
13 60
13 70
13 80
13 90
13 100
13 110
13 120
13 130
13 140
13 150
13 160
13 170
13 180
13 190
13 200
13 210
13 220
13 230
13 240
13 250
13 260
13 270
13 280
13 290
13 300
13 310
13 320
13 330
13 340
13 350
13 360
13 370
13 380
13 390
13 400
13 410
13 420
13 430
13 440
13 450
13 460
13 470
13 480
13 490
13 500
13 510
13 520
13 530
13 540
13 550
13 560
13 570
13 580
13 590
13 600
13 610
13 620
13 630
13 640
13 650
13 660
13 670
13 680
13 690
13 700
13 710
13 720
13 730
13 740
13 750
Epoch 14: train_loss: 0.0191 train_acc: 0.9944 train_prec: 0.8614 train_rec: 0.8979| val_loss: 0.9554 val_acc: 0.9419 val_prec: 0.5595 val_rec: 0.6288
00:05:44.36
14 0
14 10
14 20
14 30
14 40
14 50
14 60
14 70
14 80
14 90
14 100
14 110
14 120
14 130
14 140
14 150
14 160
14 170
14 180
14 190
14 200
14 210
14 220
14 230
14 240
14 250
14 260
14 270
14 280
14 290
14 300
14 310
14 320
14 330
14 340
14 350
14 360
14 370
14 380
14 390
14 400
14 410
14 420
14 430
14 440
14 450
14 460
14 470
14 480
14 490
14 500
14 510
14 520
14 530
14 540
14 550
14 560
14 570
14 580
14 590
14 600
14 610
14 620
14 630
14 640
14 650
14 660
14 670
14 680
14 690
14 700
14 710
14 720
14 730
14 740
14 750
Epoch 15: train_loss: 0.0151 train_acc: 0.9959 train_prec: 0.8834 train_rec: 0.9095| val_loss: 0.9010 val_acc: 0.9510 val_prec: 0.6146 val_rec: 0.6477
00:05:44.28
15 0
15 10
15 20
15 30
15 40
15 50
15 60
15 70
15 80
15 90
15 100
15 110
15 120
15 130
15 140
15 150
15 160
15 170
15 180
15 190
15 200
15 210
15 220
15 230
15 240
15 250
15 260
15 270
15 280
15 290
15 300
15 310
15 320
15 330
15 340
15 350
15 360
15 370
15 380
15 390
15 400
15 410
15 420
15 430
15 440
15 450
15 460
15 470
15 480
15 490
15 500
15 510
15 520
15 530
15 540
15 550
15 560
15 570
Namespace(tokenizer='/cluster/project/sachan/abhinav/saved_models/electra-mnli', model='/cluster/project/sachan/abhinav/saved_models/electra-mnli', weight='12', savepath='/cluster/project/sachan/abhinav/saved_models/electra-ft', finetune='F', savepath2='mnli', map='base', train_strat='1', dev_strat='1', replace_strat='char', replace_count=1)
['intentional', 'false causality', 'faulty generalization', 'fallacy of extension', 'false dilemma', 'fallacy of logic', 'appeal to emotion', 'circular reasoning', 'fallacy of relevance', 'ad populum', 'fallacy of credibility', 'equivocation', 'ad hominem']
Traceback (most recent call last):
  File "/cluster/home/alalwani/logical-fallacy/codes_for_models/abhinav_experiments/logicedu.py", line 457, in <module>
    logger.info("starting training")
  File "/cluster/home/alalwani/logical-fallacy/codes_for_models/abhinav_experiments/logicedu.py", line 307, in train
    _, prediction = model(pair_token_ids,
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py", line 974, in forward
    discriminator_hidden_states = self.electra(
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py", line 894, in forward
    hidden_states = self.encoder(
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py", line 583, in forward
    layer_outputs = layer_module(
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py", line 469, in forward
    self_attention_outputs = self.attention(
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py", line 396, in forward
    self_outputs = self.self(
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/alalwani/miniconda3/lib/python3.9/site-packages/transformers/models/electra/modeling_electra.py", line 298, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
KeyboardInterrupt
